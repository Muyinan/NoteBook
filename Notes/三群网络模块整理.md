### 1 Lua侧：

​		主要有两个重要功能，一是将.proto文件里的message结构解析为Lua的表pb_class，因此可以通过pb_class()的形式构造出Message类（重写\_\_call方法）。然后Lua侧还新建了name->Message类，id->Message类等几个映射表，这些映射表在对协议进行接收和发送时会用到。二是将Message类序列化为二进制数据后传给C++侧，或者将C++侧传来的二进制数据反序列化为Message类，这个过程底层是调用的公司封装的谷歌pb协议接口。

##### 1.0 预处理

​		解析pb协议，将其转化为Message类，并建立映射表。

```lua
-- DynamicProtobuffWrapper.lua，在该文件里解析pb协议，这里不是网络部分的重点，没有深入研究

--pb_helper.lua
for k, pb in pairs(client) do	--遍历解析后的pb协议
	for MsgName, MsgType in pairs(pb) do
		if type(MsgType) == "table" and MsgType.FindFieldDescriptor then
			local MsgID = clientIDs[MsgName]	--根据协议名查找协议id
			if MsgID then
				if l_s2cIdToS2cClass[MsgID] then
					-- 如果某proto文件import其他proto文件，会产生冗余协议，进行处理（但是代码里没有处理...）
				else
					registerS2C(MsgName, MsgID, MsgType) --注册该协议，把协议存入映射表
				end
			end
		end
	end
end

local function registerS2C (name, id, pb_class)
	--定义用 protocol buffers 解析的S2C协议，代码省略
	S2CManager.Register(id, s2c_class)	--这三行全是将协议写入映射表
	addPbInfo("S2C", name, pb_class, id)
	l_s2cIdToS2cClass[id] = s2c_class
end

local function addPbInfo (type, name, pb_class, id)	--写入映射表
	local info = {type=type, name=name, pb_class=pb_class, id=id}
	l_nameToInfo[name] = info
	l_pbClassToInfo[pb_class] = info
end

-- S2CManager.lua
def.static("number", "table").Register = function (s2cCommandId, s2cCommandClass)
    -- 对该协议进行检查，是否已经注册过等等，代码省略
	_commandMap[s2cCommandId] = s2cCommandClass	--写入映射表
end
```

##### 	1.1 协议发送

​		在发送协议时，构建Message类、获取协议id等等操作都需经过映射表来进行操作。Lua侧首先根据名字构建Message类，然后将其序列化为二进制数据，最后调用C++接口发送数据。以下是Lua发送协议的主要代码，省去了无关代码。

```lua
-- 业务代码一般这样写
local msg = pb_helper.NewCmd("MyProtocol")
--填充协议数据，代码省略
pb_helper.Send(msg)

-- pb_helper.lua
def.static("string", "=>", "table").NewCmd = function (cmdName)
    local info = l_nameToInfo[cmdName]	--根据协议名来查找协议信息
    if info then
        return info.pb_class()	--返回Message类
    else
        return nil
    end
end

def.static("table").Send = function (msg)
    local pb_class = msg:GetMessage()
    local info = l_pbClassToInfo[pb_class]
    local type, id = info.type, info.id
    --apt hook、是否需要打印协议消息等逻辑，代码省略
    if type == "S2C" then	--“S2C”类型是要发送给服务器端的协议，走下面流程
        local cmd = C2SProtocCommand.new(id, msg)
        local ECGame = require "Main.ECGame"
        ECGame.Instance().m_Network:SendGameData(cmd)	--调用ECNetwork的方法发送协议数据
    else
        --协议类型为“DS”或者其它类型，需要发送给DS等其他服务器，处理流程有稍许区别（项目里逻辑现在不会进入这里），代码省略
    end
end

-- ECNetwork.lua
def.method(C2SCommand).SendGameData = function (self, cmd)
    --是否处于协议debug模式，是则打印日志消息，代码省略
	local bw = BinaryWriter.Create()
	cmd:Marshal(bw)			 --把协议序列化为二进制数据流，里边调用了C++侧封装的谷歌api
    local g = GamedataSend() --这个是C++类，将二进制数据流保存在C++类的成员变量data里
	g.data = bw:ToOctets()	

	self:SendProtocol(g, cmd:GetType())	--进入下面的逻辑
    
	bw:Destroy()
	g.data:Destroy()
end

def.method(Protocol, "number").SendProtocol = function (self, p, ptype)
    --是否进入协议debug模式，代码省略
	local os = OctetsStream.OctetsStream()	--将二进制数据流复制到C++类OctetsStream中
	p:Marshal(os)	
	self.m_GameSession:SendLuaProtocol(ptype, os)	--调用C++接口，从这里开始逻辑正式进入C++部分
end
```

##### 	2.2 协议接收

​		Lua侧在Tick函数里不断地调用C++侧的peek方法，该方法会判断C++侧的协议接收队列是否为空，如果不为空则开始处理接收协议。处理接收协议时，C++侧会传来协议的二进制数据流和协议的id，Lua侧再根据id重构Message类，然后调用谷歌api反序列化二进制数据，并填充到Message类里。最后调用raiseEvent表示客户端接收到消息。

```lua
-- ECNetwork.lua
def.method("number").Tick = function(self, dt)
	local gs = self.m_GameSession	--C++类AzureGameSession
	gs:Tick(dt)
	--全局变量的更新,代码省略
	gs:PeekProtocols()	--调用C++接口，查看协议的存储队列是否为空，不为空则处理协议
	--是否处于debug模式，是则打印日志消息，代码省略
end

-- LuaECGameSession.cpp
static int PeekProtocols(lua_State *L)
{
	AzureGameSession* self = (AzureGameSession*)AzureHelpFuncs::GetObject(L, 1, mt_name);
	GNET::CommonData* data = nullptr;
	while(self->PopProtocol(data)) {
		if (!self->ProcessProtocol(*data)) {
			auto& os = data->GetStreamData();
			--是否debug Octets，代码省略
			lua_getglobal(L, "ProcessGameDataSend");
			lua_pushnumber(L, data->GetType());
			lua_pushlightuserdata(L, os.begin());
			lua_pushnumber(L, os.size());
			pLua->PCall(3, 1); --调用Lua侧函数ProcessGameDataSend
			-- 后处理，代码省略
		}
		data->Destroy();
	}
	return 0;
}

-- EntryPoint.lua
function _G.ProcessGameDataSend(cmd_type, cmd_begin, cmd_size)
    --一些全局变量的更新、是否暂停游戏计时等等逻辑，代码省略
	l_S2CManager.OnReceiveS2CCommandDataEx(cmd_type, cmd_begin, cmd_size)	--通知S2CManager接收到的协议
end

-- S2CManager.lua
def.static("number", "userdata", "number").OnReceiveS2CCommandDataEx = function (s2cCommandId, s2cCommandData, s2cCommandSize)
	local s2cCommand = _commandMap[s2cCommandId]	--根据协议id查找Message类
	if s2cCommand then
		local allocator = Lplus.tryget(s2cCommand, "Allocator")
		local cmd
		if allocator then
			cmd = allocator()
		else
			cmd = s2cCommand()
		end
		cmd:UnmarshalEx(s2cCommandData, s2cCommandSize)	--反序列化二进制数据，并且填充Message类的内容
		--apt hook，是否打印协议内容等逻辑，代码省略
		if not _G.pause_protocol then
			_eventManager:raiseEvent(S2CManager, cmd)
		end
	else
		warn(("unhandled S2C command #%d"):format(s2cCommandId))
	end
end	
```

### 2 C++侧

​		涉及到的文件和类较多，这里先简单地概括下大致逻辑。首先客户端会创建两个socket连接，一个用TCP连接服务器，可以接收和发送数据，客户端通过该socket与服务器交换协议数据；另一个用UDP连接本机，只能发送数据，该socket只起辅助作用，可以向本机发送消息来主动唤醒阻塞中的网络线程。此外客户端另起了一个线程，里边采用IO多路复用的select模型来管理这两个socket。

- 接收协议：当select线程检测到socket里有服务器发过来的协议二进制数据流（即可读）时，会调用Windows的socketAPI接收数据流。对该数据流先从头解析出协议的id和size，并读取对应size长度的数据，然后将该段数据存储到C++的游戏客户端AzureGameClient的协议接收队列里（该队列供前文提到的peek函数调用）。

- 发送协议：当客户端发送协议时，主线程会先把协议id放在二进制数据流的队首（变成id+data的数据流），然后将数据流放进Session的协议发送队列os里。当select线程检测到socket可写并且发送队列非空时，调用Windows的socketAPI发送数据。

C++里的逻辑涉及到多线程，而且调用的类很多，把代码按步骤列出来会异常臃肿，因此下面的代码只按照功能模块列出关键部分。

##### 2.1 初始化

​		两件事，一创建两个socket并建立连接(连接服务器的步骤在这里)，二新建线程跑select模型。

```C++
// AzureGameSession.h，lua侧调用该函数连接服务器，这里也是C++网络逻辑的入口
void ConnectToServer(const astring& addr, int port, const astring& username, const astring& password)
{
	IP = addr; Port = port; UserName = username; Password = password;

	GNET::PollIO::Init();	//里边主要做三件事：1.重置select模型里的各种fd_set（因为这些fd_set都是static变量）2.新建一个连接本机的UDPsocket 3.新起一个线程来跑select模型
	Open();	// 新建TCPsocket连接服务器
}

// gnactiveio.cpp，open函数会进到这里的逻辑，里边主要是创建了一个TCP的socket，然后将该socket保存在新的ActiveIO对象里并返回，AciveIO类的构造函数里连接服务器
ActiveIO *ActiveIO::Open(NetSession *assoc_session, const char* host, unsigned short port, unsigned int millisecToBreak){
    //创建socket，里边有IOS设备的适配逻辑，代码省略
    return s != A_INVALID_SOCKET ? new ActiveIO(s, sa, assoc_session, t) : NULL;
}

// gnactiveio.h，该类只是一个过渡类，最终的socket会存储在StreamIO里（暂时还没想明白为什么要用这个类来过渡）
ActiveIO::ActiveIO(A_SOCKET x, const SockAddr &saddr, NetSession *s, Type t) :
PollIO(x), type(t), closing(t == STREAM ? CONNECTING:CONNECTED), assoc_session(s), sa(saddr)
{
    assoc_session->LoadConfig();
    int err = NetSys::GetNetError();
    if (type != DGRAM_BROADCAST)
    {
        const sockaddr* addr = (const sockaddr*)sa;
        int ret = NetSys::Connect(fd, addr, sa.GetLen());	//把socket传入的建立与服务器的连接
        err = NetSys::GetNetError();
    }
    PollIO::WakeUp();	//调用之前新建的UDPsocket来唤醒线程，防止后边新加的
}

// 当连接上服务器后会手动delete ActiveIO对象，然后进入析构函数逻辑。析构函数里将TCPsocket的fd传给了StreamIO，StreamIO才是真正处理协议数据的IO
ActiveIO::~ActiveIO()
{
    if (type == STREAM)	//如果是TCP，创建StreamIO，否则创建DgramClientIO（因为TCP是数据流，UDP是报文段）
    {
        if (closing==CONNECTED)	//连接成功
        {
            {
                new StreamIO(fd, assoc_session);
            }
            PollIO::WakeUp();
        }
        else	// 连接失败直接关闭会话和socket
        {
            assoc_session->OnAbort();
            assoc_session->Destroy();
            NetSys::CloseSocket(fd);
        }
    }
    else
    {
        new DgramClientIO(fd, assoc_session, sa);
    }
}
```

##### 2.2 IO多路复用模型(select)

```C++
// gnpollio.cpp，select线程循环运行的就是该函数
void PollIO::Poll(int timeout)
{
	Thread::Mutex::Scoped l(locker_poll);	// 加锁
	{
		Thread::Mutex::Scoped l(locker_ionew);

		for (IOMap::const_iterator i = ionew.begin(); i != ionew.end(); ++i)
			iomap[(*i).first] = (*i).second;//新建立的socket一开始都是存储在ionew里，这里将ionew里的数据转移到iomap里
		ionew.clear();

	}

	A_SOCKET maxSocket = 0;	//每次select前将上次残留的数据重置
	fdset.clear();
	FD_ZERO(&rfds);
	FD_ZERO(&wfds);
	FD_ZERO(&efds);

	wakeup_flag = false;
	for (std::map<A_SOCKET, PollIO*>::iterator it = iomap.begin(); it != iomap.end(); ++it)
	{
		UpdateEvent(*it);//调用PollIO的UpdateEvent函数，根据IO所处的事件将其存储的socket放入对应的rfds、wfds或者efds中
		maxSocket = (std::max)(it->first, maxSocket);	//更新maxSocket变量，这个变量其实没什么实际用处，因为Windows的SelectAPI里的nfds参数只是用来与Berkeley套接字兼容，并无实际意义
	}

	wakeup_scope = true;	
	if (wakeup_flag)//当wakeup_flage为true时，令timeout=0，这样select函数便变为非阻塞函数。因此起到了唤醒线程的作用（唤醒线程只是根据代码进行的猜想，如果不对还请指出）
	{
		wakeup_scope = false;
		wakeup_flag = false;
		timeout = 0;
	}
	int nevents;
	if (timeout < 0)//根据传入的参数来设置select函数的阻塞模式，小于0设置永久等待，即阻塞；否则等待timeout毫秒后继续往后运行，三群里timeout传入值为1000，即select函数等待1s后继续运行
	{
		nevents = NetSys::Select((int)maxSocket + 1, &rfds, &wfds, &efds, NULL);
	}
	else
	{
		struct timeval tv;
		tv.tv_sec = timeout / 1000;
		tv.tv_usec = (timeout - (tv.tv_sec * 1000)) * 1000;
		nevents = NetSys::Select((int)maxSocket + 1, &rfds, &wfds, &efds, &tv);
	}
	wakeup_scope = false;
	if (nevents > 0)//select函数检测到可进行操作的socket，遍历fdset进行对应的写、读或者错误处理
		std::for_each(fdset.begin(), fdset.end(), std::ptr_fun(&TriggerEvent));
}

void PollIO::UpdateEvent(const IOMap::value_type iopair)
{
	PollIO *io = iopair.second;

	if (io == NULL)
		return;

	int event = io->UpdateEvent();	// 获取该IO当前所处的事件（在第三小节发送与接收协议里会详细说明）

	if (event == -1)	// -1表示该IO正处于关闭事件closing中，从iomap里删掉该IO
	{
		iomap[io->fd] = NULL;
		delete io;
		return;
	}

	if (event)	//event不为0，有写、读或者错误事件
	{
		if (event & POLL_STATE_IN) FD_SET(io->fd, &rfds);	//若有读事件，将该IO里的socket放入读集
		if (event & POLL_STATE_OUT) FD_SET(io->fd, &wfds);	//有写事件，放入写集
		if (event & POLL_STATE_ERR) FD_SET(io->fd, &efds);	//有错误事件，放入错误集，只有连向本机的socket才会被放入错误集，连接服务器的socket只会被放入读集或者写集
		fdset.push_back(io);	//统一放入fdset，select后遍历该集合
	}
}
```

##### 2.3 协议处理

```C++
// gnnetio.cpp，select线程里每次循环都会先调用该函数更新StreamIO当前所处的事件
int StreamIO::UpdateEvent()
{
	int event = 0;
	Thread::Mutex::Scoped l(session->locker);

	if (session->closing)
		return -1;

	if (ibuf->size())	// ibuf和obuf是Octets类指针，存储的是从服务器接收的原始二进制数据流。当size()返回值不为0时，表示有服务器发来的数据，需要进行处理了
		session->OnRecv();
	if (!session->closing)	
		session->OnSend();	// 将协议发送队列os里的数据放到obuf里，只要会话处于正常连接状态就会一直调用该函数来检查os队列
	if (obuf->size() || !(session->NoMoreData()))	//obuf不为空或者os不为空，还有协议未发给服务器端，增加OUT事件
		event = POLL_STATE_OUT;

	if (ibuf->size() < ibuf->capacity())	//ibuf没达到容量上限，仍然可以接收数据，增加IN事件
		event |= POLL_STATE_IN;
	return event;
}
```

- 协议接收

```C++
// gnproto.cpp
void Protocol::Manager::Session::OnRecv()
{
	timer.Reset();
	Octets* input = nullptr;

	try
	{
		input = &Input(); //返回ibuf加密后的数据流指针，指向Octets对象
	}
	catch (Protocol::Exception&)
	{
		sprintf(errormsg, ("Input error\n"));
		NetSession::Close();
		a_AzureLogError(errormsg);
		return;
	}

	is.insert(is.end(), input->begin(), input->end()); // 将加密后的数据流放到之前接收的数据流instream末尾
	input->clear();

	Protocol *protocol = NULL;
	Protocol::Type type;

	try
	{
		while (!is.eos())	//is没读完时，循环读取数据
		{
			uint32_t size;
			is >> Marshal::Begin >> CompactUINT(type) >> CompactUINT(size);	//解析出协议的id和size。CompactUINT是类名，继承自Marshal类，里面封装了对uint的压缩与解压接口。id和size两个数据是在压缩后放入数据流的，因此要解压后才能获取原数据
			if (size > is.size() - is.get_pos())	//如果is剩余的数据长度小于协议的size则直接结束处理。这种情况代表着协议只接收了一半，剩下一半客户端还没接收到，待接收到之后再进行处理
			{
				is >> GNET::Marshal::Rollback;	//回滚。is类里有两个void指针base和high分别指向存储数据流的头和尾，此外还有两个整型变量pos和tranpos。(*base, *(base+pos))之间的数据代表已读，每次读数据前会先用tranpos保存上次pos的位置，以便对pos进行回滚。
				break;
			}

			is >> Marshal::Rollback;

			Manager::Session::Stream data(is.session);
			is >> Marshal::Begin >> CompactUINT(type) >> data >> Marshal::Commit;	// 将数据读到data里
			if ((protocol = Protocol::Create(type)))	
			{
				// data可以认为是右值变量，所以调move方法，可能提高效率
				data.move_unmarshal(*protocol); // 把data的数据转移到protocol里
				manager->OnRecvProtocol(sid, protocol); // 把该protocol对象放入AzureGameClient的协议接收队列里
			}
		}
	}
	catch (Marshal::Exception &)
	{
		is >> Marshal::Rollback;
	}
}

Octets& Input()
{
	isec->Update(ibuffer);	// isec是Security类指针，这里在对数据进行加密，不同的加密类加密方法不一样，但三群里的加密类是NullSecurity类，即不加密，不会对原始数据进行任何处理
	isecbuf.insert(isecbuf.end(), ibuffer.begin(), ibuffer.end());
	ibuffer.clear(); //读取ibuffer的数据后需要清空ibuffer
	return isecbuf;
}
```

- 协议发送

```c++
// gnproto.h
bool Send(const Protocol *protocol, bool urg)
{
    Thread::Mutex::Scoped l(locker);
    Marshal::OctetsStream ps;
    protocol->Encode(ps);	// 将protocol的数据进行处理后放入ps里
    if(urg)	//根据传入数据判断放入os队首还是队尾
        os.push_front(ps);
    else
        os.push_back(ps);
    PollIO::WakeUp();
    return true;
}

void Protocol::Encode(Marshal::OctetsStream &os) const
{
    os << CompactUINT(type) << *this; // type是Protocol类的成员变量，存储协议的id，这里先把协议id进行压缩放入os里，然后才把存储的协议数据放入os
}

// gnproto.cpp
void Protocol::Manager::Session::OnSend()
{
	if (state->TimePolicy(timer.Elapse()))
	{
		if (manager->blCompress)
		{
			//如果采用压缩方式传送数据则进入这里的逻辑，代码省略
		}
		else
		{
			if (os.size())
			{
				Octets& data = os.front();	// data是协议发送队列outstream里的第一个Octets
                
				size_t dataSize = data.size();

				int nSent = Output(data);
				if (nSent < 0)
				{
					//	-1: data.size > capacity，应直接扔掉，否则就堵塞在这里了直到断网。。
					os.pop_front();
					// 日志输出，代码省略
				}
				else if (nSent == 0)
				{
					//	0: 这次缓存区不够发，等下次
				}
				else
				{
					//	发成功
					os.pop_front();
				}
				timer.Reset();
			}
		}

	}
	else
	{
		sprintf(errormsg, ("Local timeout\n"));
		NetSession::Close();
	}
}

// gnnetio.h
//	return:
//		< 0: data.size > capacity等原因，应直接扔掉
//		0: 这次缓存区不够发，等下次
//		1: 这次够发且塞进去了
int Output(Octets &data)
{
	if (data.size() > obuffer.capacity())
		return -1;

	if (data.size() + obuffer.size() > obuffer.capacity())
		return 0;

	osec->Update(data);
	obuffer.insert(obuffer.end(), data.begin(), data.end());
	return 1;
}
```

### 3 几点疑惑

1. 为什么要用select模型。客户端这边跑起来其实只有两个socket连接，一个连接本机一个连接服务器。为什么不直接为这两个连接创建两个线程，然后各自处理自己的收发数据。
2. 客户端接收数据时会先读取协议的id和size，然后才根据size读取对应长度的协议内容data，所以服务器发过来的单个协议数据流应该是id+size+data。相应的客户端发送时理应也按照该规则发送二进制数据，但是代码里的发送数据却是id+data，是服务器有相应的处理措施吗？还是底层用谷歌API序列化Message类时已经把size放入data的首端了？



PS：类图、流程图以及一些类的说明，这些帮助理解的内容有空时再加上

- pb_helper

​		辅助工具类，是业务代码与ECNetwork的“中间人”。封装了ECNetwork侧的协议收发接口，这些接口大多与业务代码进行交互。除此之外还用DynamicProtobufWrapper类解析了本地的.proto文件，将里面的message结构封装在了Lua表的元表里，收发协议的时候可以根据名字和id查找使用这些表。



- ECNetwork

​		主要功能是连接和释放服务器，基本上都是调用成员变量m_GameSession（C++类）的接口。ECNetwork的连接释放，以及tick检查连接是否正常等等逻辑，都是在C++里实现的，lua侧基本上只负责调用。

![image-20230106155333437](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230106155333437.png)

![image-20230106155400112](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230106155400112.png)

### C++侧：



DynamicProtobufWrapper.lua

对data/PB/下的.pb文件进行读取包装为lua的表，以便在lua里使用这些数据

唯一一个特殊处理如下，在Protocol::Manager::Session::OnSend()里调用

![image-20230111173009096](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230111173009096.png)





![image-20230111173216340](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230111173216340.png)



windows独有的高性能网络通信模式：完成端口